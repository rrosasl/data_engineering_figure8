{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "nltk.download(['stopwords','punkt','wordnet','averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///InsertDatabaseName.db')\n",
    "df = pd.read_sql_table('messages_labeled',engine)\n",
    "df = df[df.related != 2]\n",
    "\n",
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)\n",
    "\n",
    "cat_names = Y.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products      ...        \\\n",
       "0        0      0            0             0                 0      ...         \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25992,), (25992, 36))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weather update - a cold front from Cuba that c...\n",
       "1              Is the Hurricane over or is it not over\n",
       "2                      Looking for someone but no name\n",
       "3    UN reports Leogane 80-90 destroyed. Only Hospi...\n",
       "4    says: west side of Haiti, rest of the country ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(X,Y,test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17414,), (17414, 36))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25889    In southwest Sichuan province, seven people, i...\n",
       "18484    The Somalia's Transitional Government (TNG) ca...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25889</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18484</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "25889        1        0      0            1             1                 0   \n",
       "18484        1        0      0            0             0                 0   \n",
       "\n",
       "       search_and_rescue  security  military  child_alone      ...        \\\n",
       "25889                  0         0         0            0      ...         \n",
       "18484                  0         0         0            0      ...         \n",
       "\n",
       "       aid_centers  other_infrastructure  weather_related  floods  storm  \\\n",
       "25889            0                     0                1       1      1   \n",
       "18484            0                     0                0       0      0   \n",
       "\n",
       "       fire  earthquake  cold  other_weather  direct_report  \n",
       "25889     0           0     0              1              0  \n",
       "18484     0           0     0              0              0  \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_true,y_pred):\n",
    "    '''\n",
    "    ARGS:\n",
    "    > Y_true (series), dependent variable from testing set\n",
    "    > Y_pred (series), predicted value \n",
    "    \n",
    "    OUTPUT: \n",
    "    Classification report based on recall, precision, and F1 score\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 related\n",
      "1 request\n",
      "2 offer\n",
      "3 aid_related\n",
      "4 medical_help\n",
      "5 medical_products\n",
      "6 search_and_rescue\n",
      "7 security\n",
      "8 military\n",
      "9 child_alone\n",
      "10 water\n",
      "11 food\n",
      "12 shelter\n",
      "13 clothing\n",
      "14 money\n",
      "15 missing_people\n",
      "16 refugees\n",
      "17 death\n",
      "18 other_aid\n",
      "19 infrastructure_related\n",
      "20 transport\n",
      "21 buildings\n",
      "22 electricity\n",
      "23 tools\n",
      "24 hospitals\n",
      "25 shops\n",
      "26 aid_centers\n",
      "27 other_infrastructure\n",
      "28 weather_related\n",
      "29 floods\n",
      "30 storm\n",
      "31 fire\n",
      "32 earthquake\n",
      "33 cold\n",
      "34 other_weather\n",
      "35 direct_report\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(y_test):\n",
    "    print(i, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Predicted Class 0  Predicted Class 1\n",
      "Class 0               6937                157\n",
      "Class 1                871                613\n"
     ]
    }
   ],
   "source": [
    "confusion_df = pd.DataFrame(confusion_matrix(y_test.iloc[:,1],y_pred[:,1]),\n",
    "             columns=[\"Predicted Class \" + str(class_name) for class_name in [0,1]],\n",
    "             index = [\"Class \" + str(class_name) for class_name in [0,1]])\n",
    "\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10983827493261455"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "163/1484"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_rep = classification_report(y_test.iloc[:,1],y_pred[:,1])\n",
    "#print(clas_rep)\n",
    "precision,recall,fscore,support=score(y_test.iloc[:,1],y_pred[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.98      0.93      7094\n",
      "          1       0.80      0.41      0.54      1484\n",
      "\n",
      "avg / total       0.87      0.88      0.86      8578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clas_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88844775  0.7961039 ] [ 0.97786862  0.41307278] [ 0.93101597  0.54392192] [7094 1484]\n"
     ]
    }
   ],
   "source": [
    "print(precision, recall,fscore,support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.47      0.53      2036\n",
      "          1       0.85      0.91      0.88      6542\n",
      "\n",
      "avg / total       0.79      0.81      0.80      8578\n",
      "\n",
      "Precision: from the 7033 tweets labeled as related, 84.6% were actualy related\n",
      "Recall: From the 6542 tweets that were actually related, 91.0% were labeled as related \n",
      "\n",
      "-------------------------------------------------------\n",
      "1 request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.98      0.93      7094\n",
      "          1       0.80      0.41      0.54      1484\n",
      "\n",
      "avg / total       0.87      0.88      0.86      8578\n",
      "\n",
      "Precision: from the 770 tweets labeled as request, 79.6% were actualy request\n",
      "Recall: From the 1484 tweets that were actually request, 41.3% were labeled as request \n",
      "\n",
      "-------------------------------------------------------\n",
      "2 offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8542\n",
      "          1       0.00      0.00      0.00        36\n",
      "\n",
      "avg / total       0.99      1.00      0.99      8578\n",
      "\n",
      "Precision: from the 0 tweets labeled as offer, 0.0% were actualy offer\n",
      "Recall: From the 36 tweets that were actually offer, 0.0% were labeled as offer \n",
      "\n",
      "-------------------------------------------------------\n",
      "3 aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.85      0.80      5014\n",
      "          1       0.74      0.61      0.67      3564\n",
      "\n",
      "avg / total       0.75      0.75      0.75      8578\n",
      "\n",
      "Precision: from the 2945 tweets labeled as aid_related, 74.2% were actualy aid_related\n",
      "Recall: From the 3564 tweets that were actually aid_related, 61.3% were labeled as aid_related \n",
      "\n",
      "-------------------------------------------------------\n",
      "4 medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96      7888\n",
      "          1       0.57      0.07      0.13       690\n",
      "\n",
      "avg / total       0.90      0.92      0.89      8578\n",
      "\n",
      "Precision: from the 86 tweets labeled as medical_help, 57.0% were actualy medical_help\n",
      "Recall: From the 690 tweets that were actually medical_help, 7.1% were labeled as medical_help \n",
      "\n",
      "-------------------------------------------------------\n",
      "5 medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8173\n",
      "          1       0.86      0.06      0.12       405\n",
      "\n",
      "avg / total       0.95      0.96      0.94      8578\n",
      "\n",
      "Precision: from the 29 tweets labeled as medical_products, 86.2% were actualy medical_products\n",
      "Recall: From the 405 tweets that were actually medical_products, 6.2% were labeled as medical_products \n",
      "\n",
      "-------------------------------------------------------\n",
      "6 search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8353\n",
      "          1       0.43      0.09      0.15       225\n",
      "\n",
      "avg / total       0.96      0.97      0.96      8578\n",
      "\n",
      "Precision: from the 46 tweets labeled as search_and_rescue, 43.5% were actualy search_and_rescue\n",
      "Recall: From the 225 tweets that were actually search_and_rescue, 8.9% were labeled as search_and_rescue \n",
      "\n",
      "-------------------------------------------------------\n",
      "7 security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8431\n",
      "          1       0.14      0.01      0.01       147\n",
      "\n",
      "avg / total       0.97      0.98      0.97      8578\n",
      "\n",
      "Precision: from the 7 tweets labeled as security, 14.3% were actualy security\n",
      "Recall: From the 147 tweets that were actually security, 0.7% were labeled as security \n",
      "\n",
      "-------------------------------------------------------\n",
      "8 military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      8312\n",
      "          1       0.68      0.10      0.18       266\n",
      "\n",
      "avg / total       0.96      0.97      0.96      8578\n",
      "\n",
      "Precision: from the 40 tweets labeled as military, 67.5% were actualy military\n",
      "Recall: From the 266 tweets that were actually military, 10.2% were labeled as military \n",
      "\n",
      "-------------------------------------------------------\n",
      "9 child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8578\n",
      "\n",
      "avg / total       1.00      1.00      1.00      8578\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e761291588de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclas_report\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Precision: from the {y_pred2.sum()} tweets labeled as {col}, {round(precision[1]*100,1)}% were actualy {col}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Recall: From the {support[1]} tweets that were actually {col}, {round(recall[1]*100,1)}% were labeled as {col} \\n'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(y_test):\n",
    "    \n",
    "    try: \n",
    "        y_true = y_test[col]\n",
    "        y_pred2 = y_pred[:,i]\n",
    "        clas_report = classification_report(y_true, y_pred2)\n",
    "        precision,recall,fscore,support=score(y_true, y_pred2)\n",
    "\n",
    "        print(i,col)\n",
    "        print(clas_report)\n",
    "        print(f'Precision: from the {y_pred2.sum()} tweets labeled as {col}, {round(precision[1]*100,1)}% were actualy {col}')\n",
    "        print(f'Recall: From the {support[1]} tweets that were actually {col}, {round(recall[1]*100,1)}% were labeled as {col} \\n' )\n",
    "        print('-------------------------------------------------------')\n",
    "        \n",
    "    except: pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f4d4c411048>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f4d4c411048>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__estimator__n_estimators' : [200,400,1000,2000],\n",
    "    'clf__estimator__max_depth' : [10,20,50,100,None], \n",
    "    'clf__estimator__min_samples_split' : [2,5,10],\n",
    "    'clf__estimator__min_samples_leaf' : [1,2,4]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2():\n",
    "    '''\n",
    "    Returns GrindeSearchCV object as model, i.e. classifier with optimized parameters\n",
    "    \n",
    "    args: \n",
    "        None\n",
    "        \n",
    "    Returns: \n",
    "        cv : GridSearch Model Object\n",
    "\n",
    "    '''\n",
    "    pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "    ])\n",
    "    \n",
    "    parameters = {\n",
    "    #'clf__estimator__n_estimators' : [10,100],\n",
    "    #'clf__estimator__max_depth' : [10,100,None], \n",
    "    #'clf__estimator__min_samples_split' : [2,10],\n",
    "    'clf__estimator__min_samples_leaf' : [1,2]\n",
    "    }\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters,n_jobs=-1)\n",
    "    return cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    \"\"\"saves the model to the given filepath\n",
    "    Args:\n",
    "        model (scikit-learn): fitted model\n",
    "        model_filepath (string): filepath\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    pickle.dump(model.best_estimator_, open(model_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(db_filepath='InsertDatabaseName.db'):\n",
    "    '''\n",
    "    Reads database sqlite data base as a data Frame\n",
    "    Separates into independent variable X and dependent variable Y\n",
    "    \n",
    "    Args:\n",
    "        db_filepath(str): filepath to the database\n",
    "    \n",
    "    Returns: \n",
    "        X (pandas DataFrame): Independent variable\n",
    "        Y (pandas DataFrame): Dependent variable \n",
    "        cat_names (list): Y category names\n",
    "    '''\n",
    "    \n",
    "    engine = create_engine('sqlite:///' + db_filepath)\n",
    "    df = pd.read_sql_table('messages_labeled',engine)\n",
    "    df = df[df.related != 2]\n",
    "\n",
    "    X = df['message']\n",
    "    Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)\n",
    "    \n",
    "    cat_names = Y.columns.tolist()\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    INPUT: Text (string)\n",
    "    PROCESS: \n",
    "    > Lowercase\n",
    "    > word_tokenize \n",
    "    > remove stopwords\n",
    "    \n",
    "    OUTPUT: Normalized list of words\n",
    "    '''\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text) # remove special characters\n",
    "    text = text.lower() #lowercase entire text\n",
    "    words = word_tokenize(text) #Split into words\n",
    "    \n",
    "    stop_words = stopwords.words('english') # load stop words\n",
    "    words = [word for word in words if word not in stop_words] #only those words not in stop_words\n",
    "    \n",
    "    #Lemmatization & stemmization\n",
    "    \n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    \n",
    "    return stemmed\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "    Returns GrindeSearchCV object as model, i.e. classifier with optimized parameters\n",
    "    \n",
    "    args: \n",
    "        None\n",
    "        \n",
    "    Returns: \n",
    "        cv : GridSearch Model Object\n",
    "\n",
    "    '''\n",
    "    pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    parameters = {\n",
    "    #'clf__estimator__n_estimators' : [10,100],\n",
    "    #'clf__estimator__max_depth' : [10,100,None], \n",
    "    #'clf__estimator__min_samples_split' : [2,10],\n",
    "    'clf__estimator__min_samples_leaf' : [1,2]\n",
    "    }\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters,n_jobs=-1)\n",
    "    return cv\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    '''\n",
    "    Args: \n",
    "        model (classifier)\n",
    "        X_test (pandas DataFrame) : independent variables for testings=\n",
    "        y_test (pandas DataFrame) : Dependent variables with 'true' values\n",
    "        \n",
    "    Output\n",
    "        printed scores\n",
    "    '''\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    for i, col in enumerate(y_test):\n",
    "    \n",
    "        try: \n",
    "            y_true = y_test[col]\n",
    "            y_pred2 = y_pred[:,i]\n",
    "            clas_report = classification_report(y_true, y_pred2)\n",
    "            precision,recall,fscore,support=score(y_true, y_pred2)\n",
    "\n",
    "            print(i,col)\n",
    "            print(clas_report)\n",
    "            print(f'Precision: from the {y_pred2.sum()} tweets labeled as {col}, {round(precision[1]*100,1)}% were actualy {col}')\n",
    "            print(f'Recall: From the {support[1]} tweets that were actually {col}, {round(recall[1]*100,1)}% were labeled as {col} \\n' )\n",
    "            print('-------------------------------------------------------')\n",
    "\n",
    "        except: pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'clf__estimator__min_samples_leaf': [1, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.46      0.55      2027\n",
      "          1       0.85      0.93      0.89      6551\n",
      "\n",
      "avg / total       0.80      0.82      0.81      8578\n",
      "\n",
      "Precision: from the 7159 tweets labeled as related, 84.8% were actualy related\n",
      "Recall: From the 6551 tweets that were actually related, 92.7% were labeled as related \n",
      "\n",
      "-------------------------------------------------------\n",
      "1 request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.97      0.93      7112\n",
      "          1       0.79      0.44      0.57      1466\n",
      "\n",
      "avg / total       0.88      0.88      0.87      8578\n",
      "\n",
      "Precision: from the 828 tweets labeled as request, 78.5% were actualy request\n",
      "Recall: From the 1466 tweets that were actually request, 44.3% were labeled as request \n",
      "\n",
      "-------------------------------------------------------\n",
      "2 offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8543\n",
      "          1       0.00      0.00      0.00        35\n",
      "\n",
      "avg / total       0.99      1.00      0.99      8578\n",
      "\n",
      "Precision: from the 0 tweets labeled as offer, 0.0% were actualy offer\n",
      "Recall: From the 35 tweets that were actually offer, 0.0% were labeled as offer \n",
      "\n",
      "-------------------------------------------------------\n",
      "3 aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.85      0.80      5005\n",
      "          1       0.74      0.61      0.67      3573\n",
      "\n",
      "avg / total       0.75      0.75      0.74      8578\n",
      "\n",
      "Precision: from the 2948 tweets labeled as aid_related, 73.8% were actualy aid_related\n",
      "Recall: From the 3573 tweets that were actually aid_related, 60.9% were labeled as aid_related \n",
      "\n",
      "-------------------------------------------------------\n",
      "4 medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96      7886\n",
      "          1       0.60      0.07      0.13       692\n",
      "\n",
      "avg / total       0.90      0.92      0.89      8578\n",
      "\n",
      "Precision: from the 84 tweets labeled as medical_help, 59.5% were actualy medical_help\n",
      "Recall: From the 692 tweets that were actually medical_help, 7.2% were labeled as medical_help \n",
      "\n",
      "-------------------------------------------------------\n",
      "5 medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8156\n",
      "          1       0.70      0.09      0.16       422\n",
      "\n",
      "avg / total       0.94      0.95      0.94      8578\n",
      "\n",
      "Precision: from the 56 tweets labeled as medical_products, 69.6% were actualy medical_products\n",
      "Recall: From the 422 tweets that were actually medical_products, 9.2% were labeled as medical_products \n",
      "\n",
      "-------------------------------------------------------\n",
      "6 search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      8346\n",
      "          1       0.60      0.06      0.12       232\n",
      "\n",
      "avg / total       0.96      0.97      0.96      8578\n",
      "\n",
      "Precision: from the 25 tweets labeled as search_and_rescue, 60.0% were actualy search_and_rescue\n",
      "Recall: From the 232 tweets that were actually search_and_rescue, 6.5% were labeled as search_and_rescue \n",
      "\n",
      "-------------------------------------------------------\n",
      "7 security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8419\n",
      "          1       0.00      0.00      0.00       159\n",
      "\n",
      "avg / total       0.96      0.98      0.97      8578\n",
      "\n",
      "Precision: from the 3 tweets labeled as security, 0.0% were actualy security\n",
      "Recall: From the 159 tweets that were actually security, 0.0% were labeled as security \n",
      "\n",
      "-------------------------------------------------------\n",
      "8 military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      8320\n",
      "          1       0.56      0.09      0.15       258\n",
      "\n",
      "avg / total       0.96      0.97      0.96      8578\n",
      "\n",
      "Precision: from the 41 tweets labeled as military, 56.1% were actualy military\n",
      "Recall: From the 258 tweets that were actually military, 8.9% were labeled as military \n",
      "\n",
      "-------------------------------------------------------\n",
      "9 child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8578\n",
      "\n",
      "avg / total       1.00      1.00      1.00      8578\n",
      "\n",
      "10 water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8037\n",
      "          1       0.87      0.34      0.49       541\n",
      "\n",
      "avg / total       0.95      0.96      0.95      8578\n",
      "\n",
      "Precision: from the 213 tweets labeled as water, 87.3% were actualy water\n",
      "Recall: From the 541 tweets that were actually water, 34.4% were labeled as water \n",
      "\n",
      "-------------------------------------------------------\n",
      "11 food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      7651\n",
      "          1       0.82      0.47      0.59       927\n",
      "\n",
      "avg / total       0.93      0.93      0.92      8578\n",
      "\n",
      "Precision: from the 526 tweets labeled as food, 82.1% were actualy food\n",
      "Recall: From the 927 tweets that were actually food, 46.6% were labeled as food \n",
      "\n",
      "-------------------------------------------------------\n",
      "12 shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96      7830\n",
      "          1       0.80      0.22      0.34       748\n",
      "\n",
      "avg / total       0.92      0.93      0.91      8578\n",
      "\n",
      "Precision: from the 205 tweets labeled as shelter, 80.0% were actualy shelter\n",
      "Recall: From the 748 tweets that were actually shelter, 21.9% were labeled as shelter \n",
      "\n",
      "-------------------------------------------------------\n",
      "13 clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      8440\n",
      "          1       0.65      0.08      0.14       138\n",
      "\n",
      "avg / total       0.98      0.98      0.98      8578\n",
      "\n",
      "Precision: from the 17 tweets labeled as clothing, 64.7% were actualy clothing\n",
      "Recall: From the 138 tweets that were actually clothing, 8.0% were labeled as clothing \n",
      "\n",
      "-------------------------------------------------------\n",
      "14 money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8368\n",
      "          1       0.50      0.02      0.04       210\n",
      "\n",
      "avg / total       0.96      0.98      0.96      8578\n",
      "\n",
      "Precision: from the 8 tweets labeled as money, 50.0% were actualy money\n",
      "Recall: From the 210 tweets that were actually money, 1.9% were labeled as money \n",
      "\n",
      "-------------------------------------------------------\n",
      "15 missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      8475\n",
      "          1       1.00      0.01      0.02       103\n",
      "\n",
      "avg / total       0.99      0.99      0.98      8578\n",
      "\n",
      "Precision: from the 1 tweets labeled as missing_people, 100.0% were actualy missing_people\n",
      "Recall: From the 103 tweets that were actually missing_people, 1.0% were labeled as missing_people \n",
      "\n",
      "-------------------------------------------------------\n",
      "16 refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      8298\n",
      "          1       0.56      0.03      0.06       280\n",
      "\n",
      "avg / total       0.96      0.97      0.95      8578\n",
      "\n",
      "Precision: from the 16 tweets labeled as refugees, 56.2% were actualy refugees\n",
      "Recall: From the 280 tweets that were actually refugees, 3.2% were labeled as refugees \n",
      "\n",
      "-------------------------------------------------------\n",
      "17 death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8190\n",
      "          1       0.79      0.10      0.17       388\n",
      "\n",
      "avg / total       0.95      0.96      0.94      8578\n",
      "\n",
      "Precision: from the 47 tweets labeled as death, 78.7% were actualy death\n",
      "Recall: From the 388 tweets that were actually death, 9.5% were labeled as death \n",
      "\n",
      "-------------------------------------------------------\n",
      "18 other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.99      0.93      7413\n",
      "          1       0.50      0.04      0.08      1165\n",
      "\n",
      "avg / total       0.82      0.86      0.81      8578\n",
      "\n",
      "Precision: from the 100 tweets labeled as other_aid, 50.0% were actualy other_aid\n",
      "Recall: From the 1165 tweets that were actually other_aid, 4.3% were labeled as other_aid \n",
      "\n",
      "-------------------------------------------------------\n",
      "19 infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97      8017\n",
      "          1       0.44      0.01      0.01       561\n",
      "\n",
      "avg / total       0.90      0.93      0.90      8578\n",
      "\n",
      "Precision: from the 9 tweets labeled as infrastructure_related, 44.4% were actualy infrastructure_related\n",
      "Recall: From the 561 tweets that were actually infrastructure_related, 0.7% were labeled as infrastructure_related \n",
      "\n",
      "-------------------------------------------------------\n",
      "20 transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8191\n",
      "          1       0.68      0.07      0.13       387\n",
      "\n",
      "avg / total       0.95      0.96      0.94      8578\n",
      "\n",
      "Precision: from the 40 tweets labeled as transport, 67.5% were actualy transport\n",
      "Recall: From the 387 tweets that were actually transport, 7.0% were labeled as transport \n",
      "\n",
      "-------------------------------------------------------\n",
      "21 buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98      8150\n",
      "          1       0.70      0.08      0.15       428\n",
      "\n",
      "avg / total       0.94      0.95      0.93      8578\n",
      "\n",
      "Precision: from the 50 tweets labeled as buildings, 70.0% were actualy buildings\n",
      "Recall: From the 428 tweets that were actually buildings, 8.2% were labeled as buildings \n",
      "\n",
      "-------------------------------------------------------\n",
      "22 electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8418\n",
      "          1       1.00      0.03      0.06       160\n",
      "\n",
      "avg / total       0.98      0.98      0.97      8578\n",
      "\n",
      "Precision: from the 5 tweets labeled as electricity, 100.0% were actualy electricity\n",
      "Recall: From the 160 tweets that were actually electricity, 3.1% were labeled as electricity \n",
      "\n",
      "-------------------------------------------------------\n",
      "23 tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      8525\n",
      "          1       0.00      0.00      0.00        53\n",
      "\n",
      "avg / total       0.99      0.99      0.99      8578\n",
      "\n",
      "Precision: from the 2 tweets labeled as tools, 0.0% were actualy tools\n",
      "Recall: From the 53 tweets that were actually tools, 0.0% were labeled as tools \n",
      "\n",
      "-------------------------------------------------------\n",
      "24 hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      8484\n",
      "          1       0.00      0.00      0.00        94\n",
      "\n",
      "avg / total       0.98      0.99      0.98      8578\n",
      "\n",
      "Precision: from the 0 tweets labeled as hospitals, 0.0% were actualy hospitals\n",
      "Recall: From the 94 tweets that were actually hospitals, 0.0% were labeled as hospitals \n",
      "\n",
      "-------------------------------------------------------\n",
      "25 shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8538\n",
      "          1       0.00      0.00      0.00        40\n",
      "\n",
      "avg / total       0.99      1.00      0.99      8578\n",
      "\n",
      "Precision: from the 0 tweets labeled as shops, 0.0% were actualy shops\n",
      "Recall: From the 40 tweets that were actually shops, 0.0% were labeled as shops \n",
      "\n",
      "-------------------------------------------------------\n",
      "26 aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      8485\n",
      "          1       0.00      0.00      0.00        93\n",
      "\n",
      "avg / total       0.98      0.99      0.98      8578\n",
      "\n",
      "Precision: from the 1 tweets labeled as aid_centers, 0.0% were actualy aid_centers\n",
      "Recall: From the 93 tweets that were actually aid_centers, 0.0% were labeled as aid_centers \n",
      "\n",
      "-------------------------------------------------------\n",
      "27 other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8199\n",
      "          1       0.18      0.01      0.01       379\n",
      "\n",
      "avg / total       0.92      0.96      0.93      8578\n",
      "\n",
      "Precision: from the 11 tweets labeled as other_infrastructure, 18.2% were actualy other_infrastructure\n",
      "Recall: From the 379 tweets that were actually other_infrastructure, 0.5% were labeled as other_infrastructure \n",
      "\n",
      "-------------------------------------------------------\n",
      "28 weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.96      0.91      6151\n",
      "          1       0.84      0.61      0.71      2427\n",
      "\n",
      "avg / total       0.86      0.86      0.85      8578\n",
      "\n",
      "Precision: from the 1755 tweets labeled as weather_related, 84.3% were actualy weather_related\n",
      "Recall: From the 2427 tweets that were actually weather_related, 60.9% were labeled as weather_related \n",
      "\n",
      "-------------------------------------------------------\n",
      "29 floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97      7870\n",
      "          1       0.88      0.42      0.57       708\n",
      "\n",
      "avg / total       0.94      0.95      0.94      8578\n",
      "\n",
      "Precision: from the 335 tweets labeled as floods, 88.1% were actualy floods\n",
      "Recall: From the 708 tweets that were actually floods, 41.7% were labeled as floods \n",
      "\n",
      "-------------------------------------------------------\n",
      "30 storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      7767\n",
      "          1       0.75      0.41      0.53       811\n",
      "\n",
      "avg / total       0.92      0.93      0.92      8578\n",
      "\n",
      "Precision: from the 441 tweets labeled as storm, 75.1% were actualy storm\n",
      "Recall: From the 811 tweets that were actually storm, 40.8% were labeled as storm \n",
      "\n",
      "-------------------------------------------------------\n",
      "31 fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      8496\n",
      "          1       0.67      0.02      0.05        82\n",
      "\n",
      "avg / total       0.99      0.99      0.99      8578\n",
      "\n",
      "Precision: from the 3 tweets labeled as fire, 66.7% were actualy fire\n",
      "Recall: From the 82 tweets that were actually fire, 2.4% were labeled as fire \n",
      "\n",
      "-------------------------------------------------------\n",
      "32 earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98      7761\n",
      "          1       0.86      0.63      0.73       817\n",
      "\n",
      "avg / total       0.95      0.95      0.95      8578\n",
      "\n",
      "Precision: from the 596 tweets labeled as earthquake, 86.1% were actualy earthquake\n",
      "Recall: From the 817 tweets that were actually earthquake, 62.8% were labeled as earthquake \n",
      "\n",
      "-------------------------------------------------------\n",
      "33 cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8408\n",
      "          1       0.56      0.03      0.06       170\n",
      "\n",
      "avg / total       0.97      0.98      0.97      8578\n",
      "\n",
      "Precision: from the 9 tweets labeled as cold, 55.6% were actualy cold\n",
      "Recall: From the 170 tweets that were actually cold, 2.9% were labeled as cold \n",
      "\n",
      "-------------------------------------------------------\n",
      "34 other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      8098\n",
      "          1       0.47      0.04      0.07       480\n",
      "\n",
      "avg / total       0.92      0.94      0.92      8578\n",
      "\n",
      "Precision: from the 40 tweets labeled as other_weather, 47.5% were actualy other_weather\n",
      "Recall: From the 480 tweets that were actually other_weather, 4.0% were labeled as other_weather \n",
      "\n",
      "-------------------------------------------------------\n",
      "35 direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.97      0.91      6921\n",
      "          1       0.71      0.32      0.44      1657\n",
      "\n",
      "avg / total       0.83      0.84      0.82      8578\n",
      "\n",
      "Precision: from the 739 tweets labeled as direct_report, 71.0% were actualy direct_report\n",
      "Recall: From the 1657 tweets that were actually direct_report, 31.7% were labeled as direct_report \n",
      "\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-f',\n",
       " '/root/.local/share/jupyter/runtime/kernel-d65b6651-bea9-48e3-a9b6-d44c75684be7.json']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model,'/root/.local/share/jupyter/runtime/kernel-d65b6651-bea9-48e3-a9b6-d44c75684be7.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if len(sys.argv) == 3:\n",
    "        model_filepath = sys.argv[:-1]\n",
    "        db_filepath = 'InsertDatabaseName.db'\n",
    "        print('Loading data...\\n    database location: {}'.format(db_filepath))\n",
    "        X, Y = load_data(db_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "        print('Building model...')\n",
    "        model = build_model()\n",
    "\n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        print('Evaluating model...')\n",
    "        evaluate_model(model, X_test, Y_test)\n",
    "\n",
    "        print('Saving model...   model: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('You just saved the trained model')\n",
    "\n",
    "    else:\n",
    "        print('Error, file locations not correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_filepath, model_filepath = sys.argv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-f'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "    database location: InsertDatabaseName.db\n",
      "Building model...\n",
      "Training model...\n",
      "Evaluating model...\n",
      "0 related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.46      0.53      1978\n",
      "          1       0.85      0.92      0.88      6600\n",
      "\n",
      "avg / total       0.80      0.81      0.80      8578\n",
      "\n",
      "Precision: from the 7122 tweets labeled as related, 85.0% were actualy related\n",
      "Recall: From the 6600 tweets that were actually related, 91.7% were labeled as related \n",
      "\n",
      "-------------------------------------------------------\n",
      "1 request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.93      7100\n",
      "          1       0.79      0.46      0.58      1478\n",
      "\n",
      "avg / total       0.88      0.89      0.87      8578\n",
      "\n",
      "Precision: from the 861 tweets labeled as request, 79.4% were actualy request\n",
      "Recall: From the 1478 tweets that were actually request, 46.3% were labeled as request \n",
      "\n",
      "-------------------------------------------------------\n",
      "2 offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8537\n",
      "          1       0.00      0.00      0.00        41\n",
      "\n",
      "avg / total       0.99      1.00      0.99      8578\n",
      "\n",
      "Precision: from the 0 tweets labeled as offer, 0.0% were actualy offer\n",
      "Recall: From the 41 tweets that were actually offer, 0.0% were labeled as offer \n",
      "\n",
      "-------------------------------------------------------\n",
      "3 aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.85      0.79      4963\n",
      "          1       0.74      0.60      0.66      3615\n",
      "\n",
      "avg / total       0.74      0.74      0.74      8578\n",
      "\n",
      "Precision: from the 2914 tweets labeled as aid_related, 74.5% were actualy aid_related\n",
      "Recall: From the 3615 tweets that were actually aid_related, 60.0% were labeled as aid_related \n",
      "\n",
      "-------------------------------------------------------\n",
      "4 medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.99      0.96      7862\n",
      "          1       0.54      0.08      0.15       716\n",
      "\n",
      "avg / total       0.89      0.92      0.89      8578\n",
      "\n",
      "Precision: from the 111 tweets labeled as medical_help, 54.1% were actualy medical_help\n",
      "Recall: From the 716 tweets that were actually medical_help, 8.4% were labeled as medical_help \n",
      "\n",
      "-------------------------------------------------------\n",
      "5 medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      8115\n",
      "          1       0.79      0.11      0.19       463\n",
      "\n",
      "avg / total       0.94      0.95      0.93      8578\n",
      "\n",
      "Precision: from the 63 tweets labeled as medical_products, 79.4% were actualy medical_products\n",
      "Recall: From the 463 tweets that were actually medical_products, 10.8% were labeled as medical_products \n",
      "\n",
      "-------------------------------------------------------\n",
      "6 search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8351\n",
      "          1       0.59      0.12      0.20       227\n",
      "\n",
      "avg / total       0.97      0.97      0.97      8578\n",
      "\n",
      "Precision: from the 46 tweets labeled as search_and_rescue, 58.7% were actualy search_and_rescue\n",
      "Recall: From the 227 tweets that were actually search_and_rescue, 11.9% were labeled as search_and_rescue \n",
      "\n",
      "-------------------------------------------------------\n",
      "7 security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8431\n",
      "          1       0.00      0.00      0.00       147\n",
      "\n",
      "avg / total       0.97      0.98      0.97      8578\n",
      "\n",
      "Precision: from the 5 tweets labeled as security, 0.0% were actualy security\n",
      "Recall: From the 147 tweets that were actually security, 0.0% were labeled as security \n",
      "\n",
      "-------------------------------------------------------\n",
      "8 military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      8263\n",
      "          1       0.64      0.07      0.13       315\n",
      "\n",
      "avg / total       0.95      0.96      0.95      8578\n",
      "\n",
      "Precision: from the 36 tweets labeled as military, 63.9% were actualy military\n",
      "Recall: From the 315 tweets that were actually military, 7.3% were labeled as military \n",
      "\n",
      "-------------------------------------------------------\n",
      "9 child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8578\n",
      "\n",
      "avg / total       1.00      1.00      1.00      8578\n",
      "\n",
      "10 water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      7998\n",
      "          1       0.90      0.28      0.43       580\n",
      "\n",
      "avg / total       0.95      0.95      0.94      8578\n",
      "\n",
      "Precision: from the 180 tweets labeled as water, 90.0% were actualy water\n",
      "Recall: From the 580 tweets that were actually water, 27.9% were labeled as water \n",
      "\n",
      "-------------------------------------------------------\n",
      "11 food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      7586\n",
      "          1       0.84      0.49      0.62       992\n",
      "\n",
      "avg / total       0.93      0.93      0.92      8578\n",
      "\n",
      "Precision: from the 581 tweets labeled as food, 83.8% were actualy food\n",
      "Recall: From the 992 tweets that were actually food, 49.1% were labeled as food \n",
      "\n",
      "-------------------------------------------------------\n",
      "12 shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.97      7801\n",
      "          1       0.83      0.40      0.54       777\n",
      "\n",
      "avg / total       0.93      0.94      0.93      8578\n",
      "\n",
      "Precision: from the 368 tweets labeled as shelter, 83.4% were actualy shelter\n",
      "Recall: From the 777 tweets that were actually shelter, 39.5% were labeled as shelter \n",
      "\n",
      "-------------------------------------------------------\n",
      "13 clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8435\n",
      "          1       0.71      0.07      0.13       143\n",
      "\n",
      "avg / total       0.98      0.98      0.98      8578\n",
      "\n",
      "Precision: from the 14 tweets labeled as clothing, 71.4% were actualy clothing\n",
      "Recall: From the 143 tweets that were actually clothing, 7.0% were labeled as clothing \n",
      "\n",
      "-------------------------------------------------------\n",
      "14 money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8378\n",
      "          1       0.80      0.02      0.04       200\n",
      "\n",
      "avg / total       0.97      0.98      0.97      8578\n",
      "\n",
      "Precision: from the 5 tweets labeled as money, 80.0% were actualy money\n",
      "Recall: From the 200 tweets that were actually money, 2.0% were labeled as money \n",
      "\n",
      "-------------------------------------------------------\n",
      "15 missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      8481\n",
      "          1       0.00      0.00      0.00        97\n",
      "\n",
      "avg / total       0.98      0.99      0.98      8578\n",
      "\n",
      "Precision: from the 1 tweets labeled as missing_people, 0.0% were actualy missing_people\n",
      "Recall: From the 97 tweets that were actually missing_people, 0.0% were labeled as missing_people \n",
      "\n",
      "-------------------------------------------------------\n",
      "16 refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      8275\n",
      "          1       0.71      0.02      0.03       303\n",
      "\n",
      "avg / total       0.96      0.97      0.95      8578\n",
      "\n",
      "Precision: from the 7 tweets labeled as refugees, 71.4% were actualy refugees\n",
      "Recall: From the 303 tweets that were actually refugees, 1.7% were labeled as refugees \n",
      "\n",
      "-------------------------------------------------------\n",
      "17 death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8177\n",
      "          1       0.69      0.13      0.23       401\n",
      "\n",
      "avg / total       0.95      0.96      0.94      8578\n",
      "\n",
      "Precision: from the 78 tweets labeled as death, 69.2% were actualy death\n",
      "Recall: From the 401 tweets that were actually death, 13.5% were labeled as death \n",
      "\n",
      "-------------------------------------------------------\n",
      "18 other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.99      0.93      7437\n",
      "          1       0.55      0.05      0.09      1141\n",
      "\n",
      "avg / total       0.83      0.87      0.82      8578\n",
      "\n",
      "Precision: from the 107 tweets labeled as other_aid, 55.1% were actualy other_aid\n",
      "Recall: From the 1141 tweets that were actually other_aid, 5.2% were labeled as other_aid \n",
      "\n",
      "-------------------------------------------------------\n",
      "19 infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      8031\n",
      "          1       0.46      0.01      0.02       547\n",
      "\n",
      "avg / total       0.91      0.94      0.91      8578\n",
      "\n",
      "Precision: from the 13 tweets labeled as infrastructure_related, 46.2% were actualy infrastructure_related\n",
      "Recall: From the 547 tweets that were actually infrastructure_related, 1.1% were labeled as infrastructure_related \n",
      "\n",
      "-------------------------------------------------------\n",
      "20 transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8184\n",
      "          1       0.75      0.14      0.24       394\n",
      "\n",
      "avg / total       0.95      0.96      0.94      8578\n",
      "\n",
      "Precision: from the 75 tweets labeled as transport, 74.7% were actualy transport\n",
      "Recall: From the 394 tweets that were actually transport, 14.2% were labeled as transport \n",
      "\n",
      "-------------------------------------------------------\n",
      "21 buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98      8138\n",
      "          1       0.80      0.10      0.17       440\n",
      "\n",
      "avg / total       0.95      0.95      0.93      8578\n",
      "\n",
      "Precision: from the 54 tweets labeled as buildings, 79.6% were actualy buildings\n",
      "Recall: From the 440 tweets that were actually buildings, 9.8% were labeled as buildings \n",
      "\n",
      "-------------------------------------------------------\n",
      "22 electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8418\n",
      "          1       0.82      0.06      0.11       160\n",
      "\n",
      "avg / total       0.98      0.98      0.97      8578\n",
      "\n",
      "Precision: from the 11 tweets labeled as electricity, 81.8% were actualy electricity\n",
      "Recall: From the 160 tweets that were actually electricity, 5.6% were labeled as electricity \n",
      "\n",
      "-------------------------------------------------------\n",
      "23 tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      8523\n",
      "          1       0.00      0.00      0.00        55\n",
      "\n",
      "avg / total       0.99      0.99      0.99      8578\n",
      "\n",
      "Precision: from the 2 tweets labeled as tools, 0.0% were actualy tools\n",
      "Recall: From the 55 tweets that were actually tools, 0.0% were labeled as tools \n",
      "\n",
      "-------------------------------------------------------\n",
      "24 hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      8464\n",
      "          1       0.00      0.00      0.00       114\n",
      "\n",
      "avg / total       0.97      0.99      0.98      8578\n",
      "\n",
      "Precision: from the 1 tweets labeled as hospitals, 0.0% were actualy hospitals\n",
      "Recall: From the 114 tweets that were actually hospitals, 0.0% were labeled as hospitals \n",
      "\n",
      "-------------------------------------------------------\n",
      "25 shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8541\n",
      "          1       0.00      0.00      0.00        37\n",
      "\n",
      "avg / total       0.99      1.00      0.99      8578\n",
      "\n",
      "Precision: from the 0 tweets labeled as shops, 0.0% were actualy shops\n",
      "Recall: From the 37 tweets that were actually shops, 0.0% were labeled as shops \n",
      "\n",
      "-------------------------------------------------------\n",
      "26 aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      8470\n",
      "          1       0.00      0.00      0.00       108\n",
      "\n",
      "avg / total       0.97      0.99      0.98      8578\n",
      "\n",
      "Precision: from the 0 tweets labeled as aid_centers, 0.0% were actualy aid_centers\n",
      "Recall: From the 108 tweets that were actually aid_centers, 0.0% were labeled as aid_centers \n",
      "\n",
      "-------------------------------------------------------\n",
      "27 other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      8230\n",
      "          1       0.33      0.01      0.02       348\n",
      "\n",
      "avg / total       0.93      0.96      0.94      8578\n",
      "\n",
      "Precision: from the 9 tweets labeled as other_infrastructure, 33.3% were actualy other_infrastructure\n",
      "Recall: From the 348 tweets that were actually other_infrastructure, 0.9% were labeled as other_infrastructure \n",
      "\n",
      "-------------------------------------------------------\n",
      "28 weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.96      0.91      6168\n",
      "          1       0.86      0.63      0.73      2410\n",
      "\n",
      "avg / total       0.87      0.87      0.86      8578\n",
      "\n",
      "Precision: from the 1768 tweets labeled as weather_related, 86.5% were actualy weather_related\n",
      "Recall: From the 2410 tweets that were actually weather_related, 63.4% were labeled as weather_related \n",
      "\n",
      "-------------------------------------------------------\n",
      "29 floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      7857\n",
      "          1       0.89      0.44      0.59       721\n",
      "\n",
      "avg / total       0.95      0.95      0.94      8578\n",
      "\n",
      "Precision: from the 354 tweets labeled as floods, 89.0% were actualy floods\n",
      "Recall: From the 721 tweets that were actually floods, 43.7% were labeled as floods \n",
      "\n",
      "-------------------------------------------------------\n",
      "30 storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      7775\n",
      "          1       0.78      0.36      0.50       803\n",
      "\n",
      "avg / total       0.92      0.93      0.92      8578\n",
      "\n",
      "Precision: from the 376 tweets labeled as storm, 77.7% were actualy storm\n",
      "Recall: From the 803 tweets that were actually storm, 36.4% were labeled as storm \n",
      "\n",
      "-------------------------------------------------------\n",
      "31 fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      8492\n",
      "          1       0.67      0.02      0.04        86\n",
      "\n",
      "avg / total       0.99      0.99      0.99      8578\n",
      "\n",
      "Precision: from the 3 tweets labeled as fire, 66.7% were actualy fire\n",
      "Recall: From the 86 tweets that were actually fire, 2.3% were labeled as fire \n",
      "\n",
      "-------------------------------------------------------\n",
      "32 earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      7782\n",
      "          1       0.90      0.67      0.77       796\n",
      "\n",
      "avg / total       0.96      0.96      0.96      8578\n",
      "\n",
      "Precision: from the 595 tweets labeled as earthquake, 90.1% were actualy earthquake\n",
      "Recall: From the 796 tweets that were actually earthquake, 67.3% were labeled as earthquake \n",
      "\n",
      "-------------------------------------------------------\n",
      "33 cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      8406\n",
      "          1       0.71      0.10      0.17       172\n",
      "\n",
      "avg / total       0.98      0.98      0.97      8578\n",
      "\n",
      "Precision: from the 24 tweets labeled as cold, 70.8% were actualy cold\n",
      "Recall: From the 172 tweets that were actually cold, 9.9% were labeled as cold \n",
      "\n",
      "-------------------------------------------------------\n",
      "34 other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      8133\n",
      "          1       0.43      0.04      0.07       445\n",
      "\n",
      "avg / total       0.92      0.95      0.93      8578\n",
      "\n",
      "Precision: from the 37 tweets labeled as other_weather, 43.2% were actualy other_weather\n",
      "Recall: From the 445 tweets that were actually other_weather, 3.6% were labeled as other_weather \n",
      "\n",
      "-------------------------------------------------------\n",
      "35 direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.97      0.91      6895\n",
      "          1       0.73      0.31      0.44      1683\n",
      "\n",
      "avg / total       0.83      0.84      0.82      8578\n",
      "\n",
      "Precision: from the 715 tweets labeled as direct_report, 73.0% were actualy direct_report\n",
      "Recall: From the 1683 tweets that were actually direct_report, 31.0% were labeled as direct_report \n",
      "\n",
      "-------------------------------------------------------\n",
      "Saving model...   model: ['/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py', '-f']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-c942a4c9bf7f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving model...   model: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You just saved the trained model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c38d542005d1>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, model_filepath)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
